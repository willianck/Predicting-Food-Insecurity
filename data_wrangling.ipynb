{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import cross_validate as cross_validation, ShuffleSplit, cross_val_score, train_test_split, KFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/../RHoMIS_Data/Data/RHoMIS_Indicators.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-61fa1ae7817a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/../RHoMIS_Data/Data/RHoMIS_Indicators.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/../RHoMIS_Data/Data/RHoMIS_Indicators.csv'"
     ]
    }
   ],
   "source": [
    "data  = pd.read_csv('/../RHoMIS_Data/Data/RHoMIS_Indicators.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data:\n",
    "    print(data[column].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_col = ['LandCultivated', 'LandOwned', 'currency_conversion_factor','total_income_USD_PPP_pHH_Yr','offfarm_income_USD_PPP_pHH_Yr','value_livestock_prod_consumed_USD_PPP_pHH_Yr','NrofMonthsWildFoodCons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = ['Country','HouseholdType','Head_EducationLevel', 'WorstFoodSecMonth' ,'BestFoodSecMonth','HFIAS_status']\n",
    "# Head_EducationLevel specification about  possible values was not given so we omit this for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace negative values for features that are bounded to be positive only  as distance metrics like Land cultivated measured in hectares or Income and PPP earned \n",
    "\n",
    "def replace_negative(data,columns):\n",
    "    for col in columns:\n",
    "        data.loc[data[col] < 0] = 0\n",
    "    \n",
    "\n",
    "# Dictionary for months in different languange to english\n",
    "months_to_eng = { 'ukuboza': 'dec','gashyantare' : 'feb', 'kamena' : 'jun', 'mutarama': 'jan', 'nyakanga' : 'jul' ,'nzeri' : 'sep','ukwakira' : 'oct',\n",
    "                 'gicurasi' : 'may' , 'werurwe' : 'mar', 'kanama' : 'aug','ugushyingo' : 'nov' ,'mata' : 'apr'  }\n",
    "\n",
    "def process_months(var):\n",
    "    if var in months_to_eng:\n",
    "            return  months_to_eng.get(var)\n",
    "    else: return var    \n",
    "                \n",
    "translate = lambda x : process_months(x)\n",
    "\n",
    "def process_status(var):\n",
    "    if var in HFIAS_status:\n",
    "        return HFIAS_status.get(var)\n",
    "\n",
    "encode = lambda x : process_status(x)\n",
    "\n",
    "# encode ordinal data \n",
    "HFIAS_status = {'SeverelyFI':4,'ModeratelyFI':3,'MildlyFI':2,'FoodSecure':1 }\n",
    "status = ['SeverelyFI','ModeratelyFI','MildlyFI','FoodSecure']\n",
    "\n",
    "# assigning food security level to raw scores \n",
    "def discrete_assignment(score):\n",
    "    if score >= 7: return status[0]\n",
    "    elif (score == 4) or (score == 5) or (score == 6): return status[1]\n",
    "    elif (score == 2) or (score == 3): return status[2]\n",
    "    elif (score == 0) or (score == 1): return status[3]\n",
    "    else: return np.NaN \n",
    "\n",
    "fies_assignment = lambda x : discrete_assignment(x)\n",
    "\n",
    "\n",
    "def process_scales(hfias,fies):\n",
    "    if ( pd.isnull(hfias)) and (pd.isnull(fies)): return np.NaN\n",
    "    elif (pd.isnull(hfias)) and (pd.notnull(fies)): return fies\n",
    "    elif (pd.notnull(hfias)) and (pd.notnull(fies)): return hfias\n",
    "#     hfias not missing , fies missing\n",
    "    else: return hfias \n",
    "\n",
    "new_scale = lambda x : process_scales(x['HFIAS_status'],x['FIES_Score'])\n",
    "\n",
    "\n",
    "map_educationlevel = {'primary':'primary',\n",
    "                      'No_school':'no_school',\n",
    "                      'secondary':'secondary',\n",
    "                      'no_school':'no_school',\n",
    "                      'postsecondary':'postsecondary',\n",
    "                      'adulteducation':'adulteducation',\n",
    "                      'illiterate':'illiterate',\n",
    "                      'literate':'literate',\n",
    "                      'secondary_1':'secondary',\n",
    "                      'primary_2':'primary',\n",
    "                       'no school': 'no_school',\n",
    "                     'lower_secondary': 'secondary',\n",
    "                     'secondary2':'secondary',\n",
    "                      'primary_1': 'primary'}\n",
    "\n",
    "\n",
    "def process_educationlevel(var):\n",
    "    if var in map_educationlevel:\n",
    "            return  map_educationlevel.get(var)\n",
    "    else: return 'Other'    \n",
    "                \n",
    "    \n",
    "education = lambda x : process_educationlevel(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    data_model = data.copy()\n",
    "    data_model = data.drop(['ID_PROJ','ID_COUNTRY','SURVEY_ID','Region'],axis=1)\n",
    "#     replace negative values with zero \n",
    "    replace_negative(data_model,negative_col)\n",
    "    # replace  HFIAS status with 0 with missing value \n",
    "    data_model['HFIAS_status'] = data_model['HFIAS_status'].replace([0,'0'],np.NaN)\n",
    "    # replace year 0's with missing value\n",
    "    data_model['YEAR'] = data_model['YEAR'].replace([0,'0'],np.NaN)\n",
    "    #replace HHmembers and HHsizemae 0's with missing value\n",
    "    data_model['HHsizemembers'] = data_model['HHsizemembers'].replace(['0',0],np.NaN)\n",
    "    data_model['HHsizeMAE'] = data_model['HHsizeMAE'].replace([0,'0'],np.NaN)\n",
    "    # set negative values of livestock holdings to zero\n",
    "    data_model.loc[data_model['LivestockHoldings'] < 0,  'LivestockHoldings'] = 0\n",
    "    #replace WorstFoodSecMonth and BestFoodSecMonth with No_answer or none with  missing value \n",
    "\n",
    "    data_model['WorstFoodSecMonth'] = data_model['WorstFoodSecMonth'].replace(['No_answer','no_answer','None',0,'0'],np.NaN)\n",
    "    data_model['BestFoodSecMonth'] = data_model['BestFoodSecMonth'].replace(['No_answer','no_answer','None',0,'0'],np.NaN)\n",
    "#     replace HouseHold type with no answer to missing value \n",
    "    data_model['HouseholdType'] = data_model['HouseholdType'].replace(['no_answer',0,'0'],np.NaN)\n",
    "#     replace some  head_Educationlevel responsees to missing value\n",
    "    data_model['Head_EducationLevel'] = data_model['Head_EducationLevel'].replace(['No_answer','no_answer','None',0,'0'],np.NaN)\n",
    "#     translate months to english\n",
    "    data_model['BestFoodSecMonth'] = data_model.BestFoodSecMonth.apply(translate)\n",
    "    data_model['WorstFoodSecMonth'] = data_model.WorstFoodSecMonth.apply(translate)\n",
    "#     encode categorical  HFIAS status\n",
    "    data_model['HFIAS_status'] = data_model.HFIAS_status.apply(process_status)\n",
    "#     perform discrete assignments on the FIES scores \n",
    "    data_model['FIES_Score'] = data_model.FIES_Score.apply(fies_assignment)\n",
    "#     encode categorical FIES status\n",
    "    data_model['FIES_Score'] =  data_model.FIES_Score.apply(process_status)\n",
    "#    create new column representing uniform food insecurity score across dataset\n",
    "    data_model['Food_InsecurityLevel'] = data_model.apply(new_scale,axis=1)\n",
    "    data_model = data_model.drop(['HFIAS_status','FIES_Score'],axis=1)\n",
    "#     map Head_educationLevel to specific values \n",
    "    data_model['Head_EducationLevel'] = data_model['Head_EducationLevel'].apply(education)\n",
    "    return data_model\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data_preprocessing(data)\n",
    "clean_data_cp = clean_data.copy()\n",
    "clean_data_cp = clean_data.drop(clean_data_cp[clean_data_cp['Country'] == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data pefore imputation of categorical features\n",
    "\n",
    "# clean_data.to_pickle('basic_preprocessed_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute  missing categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using simple imputer \n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "clean_data_cp.BestFoodSecMonth = imputer.fit_transform(clean_data_cp['BestFoodSecMonth'].values.reshape(-1,1))[:,0]\n",
    "\n",
    "clean_data_cp.WorstFoodSecMonth = imputer.fit_transform(clean_data_cp['WorstFoodSecMonth'].values.reshape(-1,1))[:,0]\n",
    "\n",
    "clean_data_cp.Head_EducationLevel = imputer.fit_transform(clean_data_cp['Head_EducationLevel'].values.reshape(-1,1))[:,0]\n",
    "\n",
    "clean_data_cp.HouseholdType = imputer.fit_transform(clean_data_cp['HouseholdType'].values.reshape(-1,1))[:,0]\n",
    "\n",
    "#  Altitude and GPS__ALT are similar so drop one \n",
    "clean_data_cp = clean_data_cp.drop(['Altitude'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save file before encoding \n",
    "# clean_data_cp.to_pickle('preprocessed_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "\n",
    "class OneHotEncoder(SklearnOneHotEncoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(OneHotEncoder, self).__init__(**kwargs)\n",
    "        self.fit_flag = False\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        out = super().fit(X)\n",
    "        self.fit_flag = True\n",
    "        return out\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        sparse_matrix = super(OneHotEncoder, self).transform(X)\n",
    "        new_columns = self.get_new_columns(X=X)\n",
    "        d_out = pd.DataFrame(sparse_matrix.toarray(), columns=new_columns, index=X.index)\n",
    "        return d_out\n",
    "\n",
    "    def fit_transform(self, X, **kwargs):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def get_new_columns(self, X):\n",
    "        new_columns = []\n",
    "        for i, column in enumerate(X.columns):\n",
    "            j = 0\n",
    "            while j < len(self.categories_[i]):\n",
    "                new_columns.append(f'{column}_<{self.categories_[i][j]}>')\n",
    "                j += 1\n",
    "        return new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding All Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "\n",
    "categories = ['BestFoodSecMonth','WorstFoodSecMonth','Head_EducationLevel','HouseholdType','Country']\n",
    "for i in categories:\n",
    "    data_enc = encoder.fit_transform(clean_data_cp[[i]])\n",
    "    clean_data_cp = clean_data_cp.join(data_enc)\n",
    "\n",
    "clean_data_cp = clean_data_cp.drop(['BestFoodSecMonth','WorstFoodSecMonth','Head_EducationLevel','HouseholdType','Country'],axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data_cp.to_pickle('onehot_encoded_clean_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclic Encoding for Month Data and One-hot Encoding for the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "clean_data_cp['BestFoodSecMonth']= le.fit_transform(clean_data_cp['BestFoodSecMonth'])\n",
    "clean_data_cp['WorstFoodSecMonth']= le.fit_transform(clean_data_cp['WorstFoodSecMonth'])\n",
    "\n",
    "\n",
    "def cyclical_encoder(data, column):    \n",
    "    months_in_year = 12\n",
    "    sin_name = 'sin' + column\n",
    "    cos_name = 'cos' + column\n",
    "    data[sin_name] = np.sin(2*np.pi*data[column]/months_in_year)\n",
    "    data[cos_name] = np.cos(2*np.pi*data[column]/months_in_year)\n",
    "    data.drop(column, axis=1, inplace=True)\n",
    "    data.head()\n",
    "\n",
    "    \n",
    "cyclical_encoder(clean_data_cp,'BestFoodSecMonth')\n",
    "cyclical_encoder(clean_data_cp,'WorstFoodSecMonth')\n",
    "\n",
    "categories = ['Head_EducationLevel','HouseholdType','Country']\n",
    "for i in categories:\n",
    "    data_enc =  pd.DataFrame(encoder.fit_transform(clean_data_cp[[i]]))\n",
    "    clean_data_cp = clean_data_cp.join(data_enc)\n",
    "\n",
    "clean_data_cp = clean_data_cp.drop(['Head_EducationLevel','HouseholdType','Country'],axis =1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clean_data_cp['sinBestFoodSecMonth'], clean_data_cp['cosBestFoodSecMonth'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_cp.to_pickle('Cyclical_encoded_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns with missing Data and count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = pd.DataFrame(clean_data[clean_data.columns[clean_data.isnull().any()]].isnull().sum()/len(clean_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for i in range(len(missing_data)):\n",
    "        names.append(missing_data.iloc[i].name)\n",
    "values = []\n",
    "for i in range(len(missing_data)):\n",
    "        values.append(missing_data.iloc[i][0])\n",
    "\n",
    "data_1 = {'Features': names,'Missing Data Percentage': values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-95a5ec47043e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Dictionary loaded into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_1' is not defined"
     ]
    }
   ],
   "source": [
    "#plot bar chart  of missing data \n",
    "\n",
    "# Dictionary loaded into a DataFrame       \n",
    "\n",
    "df = pd.DataFrame(data=data_1)\n",
    "\n",
    " \n",
    "\n",
    "# Draw a vertical bar chart\n",
    "\n",
    "df.plot.bar(x=\"Features\", y=\"Missing Data Percentage\", title=\"Features with Missing Data\",figsize=(10,6))\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = pd.read_pickle('preprocessed_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IterativeImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-36d3e97251cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterativeImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IterativeImputer' is not defined"
     ]
    }
   ],
   "source": [
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit_transform(data_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ListWise/Case Deletion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wise_data = data_pre.copy()\n",
    "list_wise_data.dropna(inplace = True)\n",
    "percetange_observation_left = (len(list_wise_data)/ len(data_pre))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24910434393193015"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_observation_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ITERATION</th>\n",
       "      <th>ID_HH</th>\n",
       "      <th>RHoMIS_ID</th>\n",
       "      <th>GPS_LAT</th>\n",
       "      <th>GPS_LON</th>\n",
       "      <th>GPS_ALT</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>HHsizemembers</th>\n",
       "      <th>HHsizeMAE</th>\n",
       "      <th>...</th>\n",
       "      <th>Type_couple_polygamous</th>\n",
       "      <th>Type_couple_woman_works_away</th>\n",
       "      <th>Type_man_single</th>\n",
       "      <th>Type_nonparenthead</th>\n",
       "      <th>Type_other</th>\n",
       "      <th>Type_polygamous</th>\n",
       "      <th>Type_single</th>\n",
       "      <th>Type_together</th>\n",
       "      <th>Type_woman_single</th>\n",
       "      <th>Type_workaway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32480</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UG_2019_NT1_2_1</td>\n",
       "      <td>UG_2019_NT1_2_1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>31.34</td>\n",
       "      <td>1173.67</td>\n",
       "      <td>1173.671341</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32507</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UG_2019_NT1_29_1</td>\n",
       "      <td>UG_2019_NT1_29_1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>31.34</td>\n",
       "      <td>1161.70</td>\n",
       "      <td>1161.702998</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32512</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UG_2019_NT1_34_1</td>\n",
       "      <td>UG_2019_NT1_34_1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>31.32</td>\n",
       "      <td>1140.20</td>\n",
       "      <td>1140.198340</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32513</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UG_2019_NT1_35_1</td>\n",
       "      <td>UG_2019_NT1_35_1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>31.32</td>\n",
       "      <td>1131.14</td>\n",
       "      <td>1131.135762</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.79</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32514</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1</td>\n",
       "      <td>UG_2019_NT1_36_1</td>\n",
       "      <td>UG_2019_NT1_36_1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>31.32</td>\n",
       "      <td>1121.58</td>\n",
       "      <td>1121.581565</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         YEAR  ITERATION             ID_HH         RHoMIS_ID  GPS_LAT  \\\n",
       "32480  2019.0          1   UG_2019_NT1_2_1   UG_2019_NT1_2_1     1.39   \n",
       "32507  2019.0          1  UG_2019_NT1_29_1  UG_2019_NT1_29_1     1.38   \n",
       "32512  2019.0          1  UG_2019_NT1_34_1  UG_2019_NT1_34_1     1.38   \n",
       "32513  2019.0          1  UG_2019_NT1_35_1  UG_2019_NT1_35_1     1.39   \n",
       "32514  2019.0          1  UG_2019_NT1_36_1  UG_2019_NT1_36_1     1.38   \n",
       "\n",
       "       GPS_LON  GPS_ALT     Altitude  HHsizemembers  HHsizeMAE  ...  \\\n",
       "32480    31.34  1173.67  1173.671341            2.0       1.33  ...   \n",
       "32507    31.34  1161.70  1161.702998           10.0       6.65  ...   \n",
       "32512    31.32  1140.20  1140.198340           10.0       6.78  ...   \n",
       "32513    31.32  1131.14  1131.135762            9.0       5.79  ...   \n",
       "32514    31.32  1121.58  1121.581565            7.0       4.85  ...   \n",
       "\n",
       "       Type_couple_polygamous  Type_couple_woman_works_away  Type_man_single  \\\n",
       "32480                       0                             0                0   \n",
       "32507                       0                             0                0   \n",
       "32512                       0                             0                0   \n",
       "32513                       0                             0                0   \n",
       "32514                       0                             0                0   \n",
       "\n",
       "      Type_nonparenthead Type_other  Type_polygamous  Type_single  \\\n",
       "32480                  0          0                0            0   \n",
       "32507                  0          0                0            0   \n",
       "32512                  0          0                0            0   \n",
       "32513                  0          0                0            0   \n",
       "32514                  0          0                0            0   \n",
       "\n",
       "       Type_together  Type_woman_single  Type_workaway  \n",
       "32480              0                  1              0  \n",
       "32507              0                  1              0  \n",
       "32512              0                  0              0  \n",
       "32513              0                  1              0  \n",
       "32514              0                  0              0  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_wise_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
